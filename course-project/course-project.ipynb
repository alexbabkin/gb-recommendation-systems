{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Финальный проект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Для работы с матрицами\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Матричная факторизация\n",
    "from implicit import als\n",
    "\n",
    "# Бустинг\n",
    "from lightgbm import LGBMRanker\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Самостоятельно написанные классы и  функции \n",
    "from src.metrics import precision_at_k\n",
    "from src.recommenders import MainRecommender\n",
    "from src.utils import prefilter_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./retail_train.csv')\n",
    "data_test = pd.read_csv('./retail_test1.csv')\n",
    "\n",
    "item_features = pd.read_csv('./product.csv')\n",
    "user_features = pd.read_csv('./hh_demographic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "user_features.columns = [col.lower() for col in user_features.columns]\n",
    "\n",
    "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "user_features.rename(columns={'household_key': 'user_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_MATCHER_WEEKS = 6\n",
    "VAL_RANKER_WEEKS = 3\n",
    "\n",
    "# берем данные для тренировки matching модели\n",
    "data_train_matcher = data_train[data_train['week_no'] < data_train['week_no'].max() - (VAL_MATCHER_WEEKS + VAL_RANKER_WEEKS)]\n",
    "\n",
    "# берем данные для валидации matching модели\n",
    "data_val_matcher = data_train[(data_train['week_no'] >= data_train['week_no'].max() - (VAL_MATCHER_WEEKS + VAL_RANKER_WEEKS)) &\n",
    "                      (data_train['week_no'] < data_train['week_no'].max() - (VAL_RANKER_WEEKS))]\n",
    "\n",
    "\n",
    "# берем данные для тренировки ranking модели\n",
    "data_train_ranker = data_val_matcher.copy()  # Для наглядности. Далее мы добавим изменения, и они будут отличаться\n",
    "\n",
    "# берем данные для теста ranking, matching модели\n",
    "data_val_ranker = data_train[data_train['week_no'] >= data_train['week_no'].max() - VAL_RANKER_WEEKS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/alex/resommendation-systems/course-project/src/utils.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['price'] = data['sales_value'] / (np.maximum(data['quantity'], 1))\n",
      "Decreased # items from 83685 to 5001\n"
     ]
    }
   ],
   "source": [
    "n_items_before = data_train_matcher['item_id'].nunique()\n",
    "\n",
    "data_train_matcher = prefilter_items(data_train_matcher, item_features=item_features, take_n_popular=5000)\n",
    "\n",
    "n_items_after = data_train_matcher['item_id'].nunique()\n",
    "print('Decreased # items from {} to {}'.format(n_items_before, n_items_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ищем общих пользователей\n",
    "common_users = list(set(data_train_matcher.user_id.values)&(set(data_val_matcher.user_id.values))&set(data_val_ranker.user_id.values))\n",
    "\n",
    "data_train_matcher = data_train_matcher[data_train_matcher.user_id.isin(common_users)]\n",
    "data_val_matcher = data_val_matcher[data_val_matcher.user_id.isin(common_users)]\n",
    "data_train_ranker = data_train_ranker[data_train_ranker.user_id.isin(common_users)]\n",
    "data_val_ranker = data_val_ranker[data_val_ranker.user_id.isin(common_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_COL = 'actual'\n",
    "ITEM_COL = 'item_id'\n",
    "USER_COL = 'user_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval_matcher = data_val_matcher.groupby(USER_COL)[ITEM_COL].unique().reset_index()\n",
    "result_eval_matcher.columns=[USER_COL, ACTUAL_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting = [None, 'bm25', 'tfidf']\n",
    "weighting = ['bm25', 'tfidf']\n",
    "n_candidates = 50"
   ]
  },
  {
   "source": [
    "### Проверим модели матчинга"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weighting: bm25...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7751eeadf59c4a0baaeae717b809341a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/4999 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69c72243d03c4d9c9443f6de2aa93ed2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weighting: tfidf...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6dedf6a744f4fd9aeda2d8ce7ebcd22"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/4999 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ee8763542d547cab1b0f60704a1c1df"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 13min 5s, sys: 5min 57s, total: 19min 3s\nWall time: 6min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for w in weighting:\n",
    "    print(f'weighting: {w}...')\n",
    "    recommender = MainRecommender(data_train_matcher, w)\n",
    "\n",
    "    result_eval_matcher[f'own_rec {w}'] = result_eval_matcher[USER_COL].apply(lambda x: recommender.get_own_recommendations(x, N=n_candidates))\n",
    "    result_eval_matcher[f'sim_item_rec {w}'] = result_eval_matcher[USER_COL].apply(lambda x: recommender.get_similar_items_recommendation(x, N=n_candidates))\n",
    "    result_eval_matcher[f'als_rec {w}'] = result_eval_matcher[USER_COL].apply(lambda x: recommender.get_als_recommendations(x, N=n_candidates))\n",
    "    result_eval_matcher[f'sim_user_rec {w}'] = result_eval_matcher[USER_COL].apply(lambda x: recommender.get_similar_users_recommendation(x, N=n_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_PRECISION = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(df_data, top_k):\n",
    "    for col_name in df_data.columns[2:]:\n",
    "        yield col_name, df_data.apply(lambda row: precision_at_k(row[col_name], row[ACTUAL_COL], k=top_k), axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('own_rec bm25', 0.18872062663185377),\n",
       " ('als_rec tfidf', 0.14788511749347258),\n",
       " ('own_rec tfidf', 0.13357702349869452),\n",
       " ('als_rec bm25', 0.1296083550913838),\n",
       " ('sim_item_rec bm25', 0.06558746736292428),\n",
       " ('sim_item_rec tfidf', 0.06015665796344648),\n",
       " ('sim_user_rec tfidf', 0.04835509138381201),\n",
       " ('sim_user_rec bm25', 0.012845953002610967)]"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "sorted(calc_precision(result_eval_matcher, TOP_K_PRECISION), key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "source": [
    "Будем использовать own recommendation. На самом деле, я также проверял вариант без взвешивания weighting = None и он показал не самые плохие результаты (но не лучшие), но он ОЧЕНЬ  долго считается - около 7-8 часов, не знаю, с чем это связано. Когда оформлял ноутбук начисто - исключил этот вариант, чтобы не тратить время"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "709064d1f6e04490a18c11d9a7789fda"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/4999 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27183daf81804a038d9dc29e1608e8bf"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "recommender = MainRecommender(data_train_matcher, 'bm25')\n",
    "items_embendings, user_embedings = recommender.get_embeddings()\n",
    "top_popular = recommender.get_top_popular()\n",
    "candidates = result_eval_matcher[USER_COL].apply(lambda x: recommender.get_own_recommendations(x, N=n_candidates))"
   ]
  },
  {
   "source": [
    "### Модель ранжирования"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_item_features(data_train_ranker, item_features, items_embendings):\n",
    "    new_features = item_features.merge(data_train_ranker, on='item_id', how='left')\n",
    "\n",
    "    item_features = item_features.merge(items_embendings, how='left')\n",
    "\n",
    "    rare_manufacturer = item_features['manufacturer'] \\\n",
    "        .value_counts()[item_features['manufacturer'].value_counts() < 50].index\n",
    "    item_features.loc[item_features['manufacturer'].isin(rare_manufacturer), 'manufacturer'] = 999999999\n",
    "    item_features['manufacturer'] = item_features.manufacturer.astype('object')\n",
    "\n",
    "\n",
    "    mean_disc = new_features \\\n",
    "        .groupby('item_id')['coupon_disc'] \\\n",
    "        .mean() \\\n",
    "        .reset_index() \\\n",
    "        .sort_values('coupon_disc')\n",
    "    item_features = item_features.merge(mean_disc, on='item_id', how='left')    \n",
    "\n",
    "\n",
    "    items_in_department = new_features \\\n",
    "        .groupby('department')['item_id'] \\\n",
    "        .count() \\\n",
    "        .reset_index() \\\n",
    "        .sort_values('item_id', ascending=False)\n",
    "    items_in_department.rename(columns={'item_id': 'items_in_department'}, inplace=True)\n",
    "\n",
    "    sales_count_per_dep = new_features \\\n",
    "        .groupby(['department'])['quantity'] \\\n",
    "        .count() \\\n",
    "        .reset_index() \\\n",
    "        .sort_values('quantity', ascending=False)\n",
    "    sales_count_per_dep.rename(columns={'quantity': 'sales_count_per_dep'}, inplace=True)\n",
    "\n",
    "    items_in_department = items_in_department.merge(sales_count_per_dep, on='department')\n",
    "    items_in_department['qnt_of_sales_per_item_per_dep_per_week'] = (\n",
    "            items_in_department['sales_count_per_dep'] /\n",
    "            items_in_department['items_in_department'] /\n",
    "            new_features['week_no'].nunique()\n",
    "    )\n",
    "    items_in_department = items_in_department.drop(['items_in_department'], axis=1)\n",
    "    item_features = item_features.merge(items_in_department, on=['department'], how='left')\n",
    "\n",
    "    item_qnt = new_features \\\n",
    "        .groupby(['item_id'])['quantity'] \\\n",
    "        .count() \\\n",
    "        .reset_index()\n",
    "    item_qnt.rename(columns={'quantity': 'quantity_of_sales'}, inplace=True)\n",
    "\n",
    "    item_qnt['sales_count_per_week'] = item_qnt['quantity_of_sales'] / new_features['week_no'].nunique()\n",
    "    item_features = item_features.merge(item_qnt, on='item_id', how='left')\n",
    "\n",
    "    items_in_department = new_features \\\n",
    "        .groupby('sub_commodity_desc')['item_id'] \\\n",
    "        .count() \\\n",
    "        .reset_index() \\\n",
    "        .sort_values('item_id', ascending=False)\n",
    "    items_in_department.rename(columns={'item_id': 'items_in_sub_commodity_desc'}, inplace=True)\n",
    "\n",
    "    sales_count_per_dep = new_features \\\n",
    "        .groupby(['sub_commodity_desc'])['quantity'] \\\n",
    "        .count() \\\n",
    "        .reset_index() \\\n",
    "        .sort_values('quantity', ascending=False)\n",
    "    sales_count_per_dep.rename(columns={'quantity': 'qnt_of_sales_per_sub_commodity_desc'}, inplace=True)\n",
    "\n",
    "    items_in_department = items_in_department.merge(sales_count_per_dep, on='sub_commodity_desc')\n",
    "    items_in_department['qnt_of_sales_per_item_per_sub_commodity_desc_per_week'] = (\n",
    "            items_in_department['qnt_of_sales_per_sub_commodity_desc'] /\n",
    "            items_in_department['items_in_sub_commodity_desc'] /\n",
    "            new_features['week_no'].nunique()\n",
    "    )\n",
    "    items_in_department = items_in_department.drop(['items_in_sub_commodity_desc'], axis=1)\n",
    "    item_features = item_features.merge(items_in_department, on=['sub_commodity_desc'], how='left')\n",
    "\n",
    "    return item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_user_features(data_train_ranker, user_features, user_embedings):\n",
    "    data_train_ranker['price'] = data_train_ranker['sales_value'] / data_train_ranker['quantity']\n",
    "    new_user_features = user_features.merge(data_train_ranker, on='user_id', how='left')\n",
    "\n",
    "    user_features = user_features.merge(user_embedings, how='left')\n",
    "\n",
    "    time = new_user_features \\\n",
    "        .groupby('user_id')['trans_time'] \\\n",
    "        .mean() \\\n",
    "        .reset_index()\n",
    "    time.rename(columns={'trans_time': 'mean_time'}, inplace=True)\n",
    "    time = time.astype(np.float32)\n",
    "    user_features = user_features.merge(time, how='left')\n",
    "\n",
    "\n",
    "    # Age\n",
    "    user_features['age'] = user_features['age_desc'].replace({\n",
    "        '65+'  : 70, \n",
    "        '45-54': 50, \n",
    "        '25-34': 30, \n",
    "        '35-44': 40, \n",
    "        '19-24': 20, \n",
    "        '55-64': 60}\n",
    "    )\n",
    "    user_features = user_features.drop('age_desc', axis=1)\n",
    "\n",
    "\n",
    "    # Income\n",
    "    user_features['income'] = user_features['income_desc'].replace({\n",
    "        '35-49K'   : 45,\n",
    "        '50-74K'   : 70,\n",
    "        '25-34K'   : 30,\n",
    "        '75-99K'   : 95,\n",
    "        'Under 15K': 15,\n",
    "        '100-124K' : 120,\n",
    "        '15-24K'   : 20,\n",
    "        '125-149K' : 145,\n",
    "        '150-174K' : 170,\n",
    "        '250K+'    : 250,\n",
    "        '175-199K' : 195,\n",
    "        '200-249K' : 245}\n",
    "    )\n",
    "    user_features = user_features.drop('income_desc', axis=1)\n",
    "\n",
    "\n",
    "    # Childrens\n",
    "    user_features['children'] = 0\n",
    "    user_features.loc[(user_features['kid_category_desc'] == '1'), 'children'] = 1\n",
    "    user_features.loc[(user_features['kid_category_desc'] == '2'), 'children'] = 2\n",
    "    user_features.loc[(user_features['kid_category_desc'] == '3'), 'children'] = 3\n",
    "    user_features = user_features.drop('kid_category_desc', axis=1)\n",
    "\n",
    "\n",
    "    # Средний чек, средний чек в неделю\n",
    "    basket = new_user_features \\\n",
    "        .groupby(['user_id'])['price'] \\\n",
    "        .sum() \\\n",
    "        .reset_index()\n",
    "\n",
    "    baskets = new_user_features.groupby('user_id')['basket_id'] \\\n",
    "        .count()\\\n",
    "        .reset_index()\n",
    "    baskets.rename(columns={'basket_id': 'baskets'}, inplace=True)\n",
    "\n",
    "    avg_basket = basket.merge(baskets)\n",
    "\n",
    "    avg_basket['avg_basket'] = avg_basket['price'] / avg_basket['baskets']\n",
    "    avg_basket['sum_per_week'] = avg_basket['price'] / new_user_features['week_no'].nunique()\n",
    "\n",
    "    avg_basket = avg_basket.drop(['price', 'baskets'], axis=1)\n",
    "    user_features = user_features.merge(avg_basket, how='left')\n",
    "\n",
    "    return user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranker_targets(data_train_ranker, data_train_matcher, candidates, N):\n",
    "    users_ranker = pd.DataFrame(data_train_ranker['user_id'].unique())\n",
    "\n",
    "    users_ranker.columns = ['user_id']\n",
    "\n",
    "    train_users = data_train_matcher['user_id'].unique()\n",
    "    users_ranker = users_ranker[users_ranker['user_id'].isin(train_users)]\n",
    "\n",
    "    # Рекомендации на основе собственных покупок\n",
    "    users_ranker = users_ranker.copy()\n",
    "    users_ranker['candidates'] = candidates\n",
    "\n",
    "    s = users_ranker.apply(\n",
    "        lambda x: pd.Series(x['candidates']), axis=1\n",
    "    ).stack().reset_index(level=1, drop=True)\n",
    "\n",
    "    s.name = 'item_id'\n",
    "\n",
    "    users_ranker = users_ranker.drop('candidates', axis=1).join(s)\n",
    "\n",
    "    users_ranker['flag'] = 1\n",
    "\n",
    "\n",
    "    ranker_targets = data_train_ranker[['user_id', 'item_id']].copy()\n",
    "\n",
    "    ranker_targets['target'] = 1 \n",
    "\n",
    "    ranker_targets = users_ranker.merge(ranker_targets, on=['user_id', 'item_id'], how='left')\n",
    "\n",
    "    ranker_targets['target'].fillna(0, inplace=True)\n",
    "    ranker_targets.drop('flag', axis=1, inplace=True)\n",
    "\n",
    "    return ranker_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_user_item_features(data_train_ranker, data_train_matcher, candiadates, item_features, user_features, items_embendings, user_embedings, N=50):\n",
    "\n",
    "    target = get_ranker_targets(data_train_ranker, data_train_matcher, candiadates, N)\n",
    "    user_features = get_extended_user_features(data_train_ranker, user_features, user_embedings)\n",
    "    item_features = get_extended_item_features(data_train_ranker, item_features, items_embendings)\n",
    "    item_features = data_train_ranker.merge(item_features, on='item_id', how='left')\n",
    "\n",
    "    new_data = item_features.merge(user_features, on='user_id', how='left')\n",
    "\n",
    "    count_perch = new_data\\\n",
    "        .groupby(['user_id', 'commodity_desc', 'week_no']) \\\n",
    "        .agg({'quantity': 'mean'}) \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'quantity': 'count_purchases_week_dep'})\n",
    "\n",
    "    mean_count_perch = new_data.groupby(['commodity_desc', 'week_no']) \\\n",
    "        .agg({'quantity': 'sum'}) \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns=({'quantity': 'mean_count_purchases_week_dep'}))\n",
    "\n",
    "    coef = count_perch.merge(mean_count_perch, on=['commodity_desc', 'week_no'], how='left')\n",
    "    coef['count_purchases_week_mean'] = coef['count_purchases_week_dep'] / coef['mean_count_purchases_week_dep']\n",
    "    coef = coef[['user_id', 'commodity_desc', 'count_purchases_week_mean']]\n",
    "\n",
    "    temp = coef\\\n",
    "        .groupby(['user_id', 'commodity_desc']) \\\n",
    "        .agg({'count_purchases_week_mean': 'mean'}) \\\n",
    "        .reset_index()\n",
    "\n",
    "    new_data = new_data.merge(temp, on=['user_id', 'commodity_desc'], how='left')\n",
    "\n",
    "    count_perch = new_data \\\n",
    "        .groupby(['user_id', 'commodity_desc', 'week_no']) \\\n",
    "        .agg({'price': 'sum'}) \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'price': 'price_week'})\n",
    "\n",
    "    mean_count_perch = new_data \\\n",
    "        .groupby(['commodity_desc', 'week_no'])\\\n",
    "        .agg({'price': 'sum'}) \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns=({'price': 'mean_price_week'}))\n",
    "\n",
    "    coef = count_perch.merge(mean_count_perch, on=['commodity_desc', 'week_no'], how='left')\n",
    "    coef['sum_purchases_week_mean'] = coef['price_week'] / coef['mean_price_week']\n",
    "    coef = coef[['user_id', 'commodity_desc', 'sum_purchases_week_mean']]\n",
    "\n",
    "    temp = coef \\\n",
    "        .groupby(['user_id', 'commodity_desc']) \\\n",
    "        .agg({'sum_purchases_week_mean': 'mean'}) \\\n",
    "        .reset_index()\n",
    "\n",
    "    new_data = new_data.merge(temp, on=['user_id', 'commodity_desc'], how='left')\n",
    "\n",
    "    new_data = new_data.merge(target, on=['item_id', 'user_id'], how='left')\n",
    "    new_data = new_data.fillna(0)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   user_id    basket_id  day  item_id  quantity  sales_value  store_id  \\\n",
       "0     2070  40618492260  594  1019940         1         1.00       311   \n",
       "1     2021  40618753059  594   840361         1         0.99       443   \n",
       "2     2021  40618753059  594   856060         1         1.77       443   \n",
       "3     2021  40618753059  594   869344         1         1.67       443   \n",
       "4     2021  40618753059  594   896862         2         5.00       443   \n",
       "\n",
       "   retail_disc  trans_time  week_no  ...      19_y    mean_time   age  income  \\\n",
       "0        -0.29          40       86  ...  2.893883  1274.421509  50.0    70.0   \n",
       "1         0.00         101       86  ...  0.000000     0.000000   0.0     0.0   \n",
       "2        -0.09         101       86  ...  0.000000     0.000000   0.0     0.0   \n",
       "3        -0.22         101       86  ...  0.000000     0.000000   0.0     0.0   \n",
       "4        -2.98         101       86  ...  0.000000     0.000000   0.0     0.0   \n",
       "\n",
       "  children avg_basket sum_per_week count_purchases_week_mean  \\\n",
       "0      0.0   2.290045     77.86153                  0.000685   \n",
       "1      0.0   0.000000      0.00000                  0.002571   \n",
       "2      0.0   0.000000      0.00000                  0.002721   \n",
       "3      0.0   0.000000      0.00000                  0.003986   \n",
       "4      0.0   0.000000      0.00000                  0.011370   \n",
       "\n",
       "  sum_purchases_week_mean  target  \n",
       "0                0.002868     0.0  \n",
       "1                0.002630     0.0  \n",
       "2                0.002794     0.0  \n",
       "3                0.005455     0.0  \n",
       "4                0.007395     0.0  \n",
       "\n",
       "[5 rows x 79 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>basket_id</th>\n      <th>day</th>\n      <th>item_id</th>\n      <th>quantity</th>\n      <th>sales_value</th>\n      <th>store_id</th>\n      <th>retail_disc</th>\n      <th>trans_time</th>\n      <th>week_no</th>\n      <th>...</th>\n      <th>19_y</th>\n      <th>mean_time</th>\n      <th>age</th>\n      <th>income</th>\n      <th>children</th>\n      <th>avg_basket</th>\n      <th>sum_per_week</th>\n      <th>count_purchases_week_mean</th>\n      <th>sum_purchases_week_mean</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2070</td>\n      <td>40618492260</td>\n      <td>594</td>\n      <td>1019940</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>311</td>\n      <td>-0.29</td>\n      <td>40</td>\n      <td>86</td>\n      <td>...</td>\n      <td>2.893883</td>\n      <td>1274.421509</td>\n      <td>50.0</td>\n      <td>70.0</td>\n      <td>0.0</td>\n      <td>2.290045</td>\n      <td>77.86153</td>\n      <td>0.000685</td>\n      <td>0.002868</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021</td>\n      <td>40618753059</td>\n      <td>594</td>\n      <td>840361</td>\n      <td>1</td>\n      <td>0.99</td>\n      <td>443</td>\n      <td>0.00</td>\n      <td>101</td>\n      <td>86</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.002571</td>\n      <td>0.002630</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021</td>\n      <td>40618753059</td>\n      <td>594</td>\n      <td>856060</td>\n      <td>1</td>\n      <td>1.77</td>\n      <td>443</td>\n      <td>-0.09</td>\n      <td>101</td>\n      <td>86</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.002721</td>\n      <td>0.002794</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021</td>\n      <td>40618753059</td>\n      <td>594</td>\n      <td>869344</td>\n      <td>1</td>\n      <td>1.67</td>\n      <td>443</td>\n      <td>-0.22</td>\n      <td>101</td>\n      <td>86</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.003986</td>\n      <td>0.005455</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021</td>\n      <td>40618753059</td>\n      <td>594</td>\n      <td>896862</td>\n      <td>2</td>\n      <td>5.00</td>\n      <td>443</td>\n      <td>-2.98</td>\n      <td>101</td>\n      <td>86</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.011370</td>\n      <td>0.007395</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 79 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "train = get_extended_user_item_features(data_train_ranker, data_train_matcher, candidates, item_features, user_features, items_embendings, user_embedings, n_candidates)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['target'], axis=1)\n",
    "y_train = train[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = []\n",
    "for col in X_train.columns:\n",
    "    if(X_train[col].dtype == np.object):\n",
    "          cat_features.append(col)\n",
    "            \n",
    "X_train[cat_features + ['user_id', 'item_id']] = X_train[cat_features + ['user_id', 'item_id']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_extended_user_item_features(data_test, data_train_matcher, candidates, item_features, user_features, items_embendings, user_embedings, n_candidates)\n",
    "X_test = test.drop(['target'], axis=1)\n",
    "y_test = test[['target']]\n",
    "X_test[cat_features + ['user_id', 'item_id']] = X_test[cat_features + ['user_id', 'item_id']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_features(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    feature = list(zip(X_train.columns.tolist(), model.feature_importances_))\n",
    "    feature = pd.DataFrame(feature, columns=['feature', 'value'])\n",
    "    features = feature.loc[feature.value > 0, 'feature'].tolist()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/alex/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/home/alex/miniconda3/envs/py36/lib/python3.6/site-packages/lightgbm/basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
      "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(objective='binary', max_depth=7, categorical_column=cat_features)\n",
    "important_features = get_important_features(lgb, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/alex/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(*args, **kwargs)\n/home/alex/miniconda3/envs/py36/lib/python3.6/site-packages/lightgbm/basic.py:1077: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\nPlease use categorical_feature argument of the Dataset constructor to pass this parameter.\n  .format(key))\n/home/alex/miniconda3/envs/py36/lib/python3.6/site-packages/lightgbm/basic.py:1098: UserWarning: categorical_feature in param dict is overridden.\n  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMClassifier(categorical_feature=['department', 'brand', 'commodity_desc',\n",
       "                                    'sub_commodity_desc',\n",
       "                                    'curr_size_of_product',\n",
       "                                    'marital_status_code', 'homeowner_desc',\n",
       "                                    'hh_comp_desc', 'household_size_desc'],\n",
       "               max_depth=7, objective='binary')"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "lgb = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    max_depth=7,\n",
    "    categorical_feature=cat_features\n",
    ")\n",
    "lgb.fit(X_train[important_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lgb.predict(X_test[important_features])\n",
    "test_preds_proba = lgb.predict_proba(X_test[important_features])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_recomendations(X_test, test_preds_proba, data_train, top_popular, item_features):\n",
    "    X_test['predict_proba'] = test_preds_proba\n",
    "    X_test.sort_values(['user_id', 'predict_proba'], ascending=False, inplace=True)\n",
    "\n",
    "    recomendations = []\n",
    "    for user, preds in X_test.groupby('user_id')['item_id']:\n",
    "        recomendations.append({'user_id': user, 'recomendations': preds.tolist()})\n",
    "    recomendations = pd.DataFrame(recomendations)\n",
    "\n",
    "    result = data_train.groupby('user_id')['item_id'].unique().reset_index()\n",
    "    result.columns = ['user_id', 'actual']\n",
    "\n",
    "    result = result.merge(recomendations, how='left')\n",
    "    \n",
    "    result.loc[result['recomendations'].isnull(), 'recomendations'] = pd.Series( [top_popular] * len(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_final_recomendations(X_test, test_preds_proba, data_train, top_popular, item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4720488195278111"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "test_score = result.apply(lambda row: precision_at_k(row['recomendations'], row['actual']), axis=1).mean()\n",
    "test_score"
   ]
  },
  {
   "source": [
    "По-моему, получился слишком хороший результат. Возможно я что-то упустил и не так посчитал"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}